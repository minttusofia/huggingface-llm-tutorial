{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that PyTorch is using the GPU.\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables, this must be done before importing transformers.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import functools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "if \"TRANSFORMERS_OFFLINE\" in os.environ and int(os.environ[\"TRANSFORMERS_OFFLINE\"]):\n",
    "    print(\"Using cached models from\", os.environ[\"HF_HOME\"])\n",
    "else:\n",
    "    print(\"Loading model from huggingface hub and saving to\", os.environ[\"HF_HOME\"])\n",
    "\n",
    "# Optional: disable warnings.\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Llama-3\n",
    "\n",
    "Meta's Llama-3 is one of the state-of-the-art open-source language models. We will use the 8 billion parameter instruction-tuned version in this tutorial.  \n",
    "Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n",
    "\n",
    "Llama-3 Instruct models can be prompted using a chat template: a prompt is a list of messages, each of which has a role (one of \"system\", \"user\" or \"assistant\") and content. System messages are meta-prompts that are used to define instructions that are hidden from the end user, whereas the user and assistant (the LLM) are taking turns in a chat dialogue.\n",
    "\n",
    "You can run conversational inference using the Transformers pipeline abstraction, or by leveraging the Auto classes with the generate() function.\n",
    "\n",
    "**Note:** Make sure to set the `HF_TOKEN` environment variable to access Llama-3 (unless you are running on Puhti or Mahti, where Llama-3-8B-Instruct is already downloaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you are done with the pipeline, delete the variable to free up GPU memory for the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipeline  # Free up GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-3\n",
    "\n",
    "We will now fine-tune Llama-3-8B on a new dataset.\n",
    "\n",
    "We would like the model to reproduce human users' preferences over two possible completions: given `(prompt, reponse A, response B)`, it should predict A, B, or tie according to what a human user preferred. We will use real human preference data collected on the Chatbot arena ([lmsys-arena-human-preference-55k](https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k)). This dataset is hosted by Huggingface as part of a vast library of datasets, which you can explore at https://huggingface.co/datasets.\n",
    "\n",
    "Note that this prediction problem could also be modeled as a classification task using e.g. a BERT-like model, but we will use Llama to demonstrate general autoregressive language model fine-tuning. A general model like Llama will be a good starting point for developing more flexible user models with LLMs: models that give their preferences in natural language, in scalar reward values, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format fine-tuning prompt-reponse pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only load a subset of the dataset to keep training time reasonable for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lmsys/lmsys-arena-human-preference-55k\"\n",
    "dataset = load_dataset(dataset_name, split='train[:2000]')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_list_from_str_column(text):\n",
    "    lst = []\n",
    "    try:\n",
    "        lst = json.loads(text)\n",
    "        if len(lst) > 0 and lst[-1] is None:\n",
    "            lst = lst[:-1]\n",
    "    except _ as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        print()\n",
    "    return lst\n",
    "\n",
    "\n",
    "def format_prompt(example):\n",
    "    preference_prompt = 'Which response to the following prompt is better? Answer with \"A\", \"B\" or \"tie\" only.'\n",
    "    answer = \"A\" if example[\"winner_model_a\"] else (\"B\" if example[\"winner_model_b\"] else \"tie\")\n",
    "    prompts = parse_list_from_str_column(example[\"prompt\"])\n",
    "    responses_a = parse_list_from_str_column(example[\"response_a\"])\n",
    "    responses_b = parse_list_from_str_column(example[\"response_b\"])\n",
    "    full_prompt = (\n",
    "        preference_prompt\n",
    "        + \"\\n\\nUser prompt:\\n\" + \"\\n\\n\".join(prompts)\n",
    "        + \"\\n\\nResponse A:\\n\" + \"\\n\\n\".join(responses_a)\n",
    "        + \"\\n\\nResponse B:\\n\" + \"\\n\\n\".join(responses_b))\n",
    "    example[\"preference_prompt\"] = full_prompt\n",
    "    example[\"preference_label\"] = answer\n",
    "    return example\n",
    "\n",
    "\n",
    "def is_single_turn_dialogue(example):\n",
    "    prompts = parse_list_from_str_column(example[\"prompt\"])\n",
    "    responses_a = parse_list_from_str_column(example[\"response_a\"])\n",
    "    responses_b = parse_list_from_str_column(example[\"response_b\"])\n",
    "    is_single_turn = len(prompts) == 1 and len(responses_a) == 1 and len(responses_b) == 1\n",
    "    if is_single_turn:\n",
    "        is_null = prompts[0] == \"null\" or responses_a[0] == \"null\" or responses_b[0] == \"null\"\n",
    "        if is_null:\n",
    "            print(\"IS NULL\")\n",
    "            print(prompts, responses_a, responses_b)\n",
    "        return not is_null\n",
    "    return False\n",
    "\n",
    "\n",
    "# Simplify the task to one-turn dialogues.\n",
    "prompt_dataset = dataset.filter(is_single_turn_dialogue)\n",
    "\n",
    "# Format the dataset into (prompt, answer) pairs.\n",
    "prompt_dataset = prompt_dataset.map(format_prompt)\n",
    "\n",
    "print('Original dataset length', len(dataset), 'filtered length', len(prompt_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_dataset[0][\"preference_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_dataset[0][\"preference_label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run Llama's tokenizer on the data once. To do this, we need a way to convert a list of chat messages from the chat template into a string prompt.\n",
    "\n",
    "Each instruction-tuned model has been trained with a specific template for turning lists of messages into a prompt string.  \n",
    "For Llama-3, this is documented in its model card at https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_format_finished_chat(messages):\n",
    "    \"\"\"Format documented in https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/.\"\"\"\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\"\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong message role {msg['role']}.\")\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def preference_data_to_finished_chat(example):\n",
    "    prompt = example[\"preference_prompt\"]\n",
    "    completion = example[\"preference_label\"]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": completion}\n",
    "    ]\n",
    "    input_text = llama_format_finished_chat(messages)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input pipeline needs to differentiate between the prompt and the completion: both need to be concatenated and passed as input to the model in training, but the cross-entropy loss should only be applied to the completion.\n",
    "\n",
    "This can be implemented using standard Huggingface Trainers by setting the \"labels\" of the corresponding tokens to -100 (the \"ignore index\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_continue_chat_prompt(messages):\n",
    "    prompt = llama_format_finished_chat(messages)\n",
    "    # Add the header for a new assistant message.\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def preference_data_to_chat_prompt(example):\n",
    "    prompt = example[\"preference_prompt\"]\n",
    "    completion = example[\"preference_label\"]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    input_text = llama_continue_chat_prompt(messages)\n",
    "    return input_text\n",
    "\n",
    "\n",
    "def tokenize_unpadded(example, tokenizer):\n",
    "    prompt_and_completion = preference_data_to_finished_chat(example)\n",
    "    encoding = tokenizer(prompt_and_completion, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create labels, with -100 for prompt tokens and actual tokens for completion\n",
    "    input_ids = encoding[\"input_ids\"].squeeze()\n",
    "    labels = input_ids.clone()\n",
    "    prompt_only = preference_data_to_chat_prompt(example)\n",
    "    prompt_encoding = tokenizer(prompt_only, return_tensors=\"pt\")\n",
    "    prompt_length = len(prompt_encoding[\"input_ids\"].squeeze())\n",
    "    labels[:prompt_length] = -100  # Ignore prompt part in loss computation\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = prompt_dataset.map(tokenize_unpadded, fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computational efficiency, filter out very long examples.\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) < 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = (tokenized_dataset\n",
    "    .shuffle(seed=123)\n",
    "    .train_test_split(test_size=0.1, shuffle=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these filtering steps, we will ultimately train on about 1400 prompt-response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_val_split[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_val_split[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation data to only include relevant columns.\n",
    "columns_to_drop = [k for k in tokenized_dataset[0].keys() if k not in [\"input_ids\", \"attention_mask\", \"labels\"]]\n",
    "train_tokens = train_val_split[\"train\"].remove_columns(columns_to_drop)\n",
    "val_tokens = train_val_split[\"test\"].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT: Parameter-efficient fine-tuning\n",
    "\n",
    "Instead of fine-tuning all 8 billion parameters, which would be very computationally expensive, we will use LoRA, low-rank adaptation, to reduce the computational cost.  \n",
    "To make fine-tuning more efficient, LoRA represents weight updates with two smaller matrices (called update matrices) through low-rank decomposition: https://arxiv.org/abs/2106.09685.\n",
    "\n",
    "We will further speed up training by quantizing model weights from their original float32 data type to 4-bit floating points. Quantization together with low-rank adaptation is referred to as QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16)\n",
    "init_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                  quantization_config=quantization_config,\n",
    "                                                  low_cpu_mem_usage=True,\n",
    "                                                  device_map=\"auto\",\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in init_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the names and shapes of trainable parameters in a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "    model: A Hugging Face model instance.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable_params: {trainable_params}\")\n",
    "    print(f\"all_params: {all_params}\")\n",
    "    \n",
    "print_trainable_parameters(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r = 8 defines the rank of the adaptation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(init_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using QLoRA, we have reduced the trainable parameters from 8 billion to 3.4 million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Trainer\n",
    "\n",
    "Huggingface transformers defines a Trainer API that makes it convenient to set up training and evaluation tasks.\n",
    "\n",
    "In this tutorial, we will use the specialized Supervised Fine-Tuning (SFT) trainer from TRL (Transformers  Reinforcement Learning) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 1e-4\n",
    "grad_acc_steps = 16\n",
    "trained_model_id = f\"{model_id}_lora_4bit_lr{learning_rate}_acc{grad_acc_steps}_1\".replace(\"/\", \"__\")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    trained_model_id,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_acc_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    max_seq_length=1024,\n",
    "    fp16=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    peft_model,\n",
    "    train_dataset=train_tokens,\n",
    "    eval_dataset=val_tokens,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run evaluation in isolation, such as before training:\n",
    "# eval_results = trainer.evaluate()\n",
    "# eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to evaluating language modeling loss (cross entropy),\n",
    "# we implement the below functions to measure accuracy of the multiple-choice predictions.\n",
    "def predict_with_model(model, val_tokens, generation_kwargs=None):\n",
    "    val_token_predictions = []\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    # Use deterministic sampling (by default) in order to get a deterministic accuracy score.\n",
    "    generation_kwargs = ({\"max_new_tokens\": 10, \"do_sample\": False, \"top_p\": None, \"temperature\": None}\n",
    "                         if generation_kwargs is None else generation_kwargs)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(val_tokens):\n",
    "            labels = torch.Tensor(example[\"labels\"]).int()\n",
    "            is_prompt = labels == -100\n",
    "            input_ids = torch.Tensor(example[\"input_ids\"])[is_prompt].int().unsqueeze(0)\n",
    "            input_ids = input_ids.to(model.device)\n",
    "            attention_mask = torch.Tensor(example[\"attention_mask\"])[is_prompt].int().unsqueeze(0)\n",
    "            attention_mask = attention_mask.to(model.device)\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, **generation_kwargs)\n",
    "            new_outputs = outputs[0, input_ids.shape[1]:]\n",
    "            val_token_predictions.append(new_outputs)\n",
    "            generated_text = tokenizer.decode(new_outputs, skip_special_tokens=True)\n",
    "            val_predictions.append(generated_text)\n",
    "            label_text = tokenizer.decode(labels[~is_prompt], skip_special_tokens=True)\n",
    "            val_labels.append(label_text)\n",
    "    model.train()\n",
    "    return val_token_predictions, val_predictions, val_labels\n",
    "\n",
    "\n",
    "def get_prediction_accuracy(labels, predictions, loose_matching=False):\n",
    "    zipped = zip(labels, predictions)\n",
    "    length = min(len(labels), len(predictions))\n",
    "    if loose_matching:\n",
    "        n_correct = np.sum([l == p[:len(l)] for l, p in zipped])\n",
    "    else:\n",
    "        n_correct = np.sum([l == p for l, p in zipped])\n",
    "    return n_correct / length\n",
    "\n",
    "\n",
    "def count_label_distribution(labels):\n",
    "    counts = {}\n",
    "    for pred in labels:\n",
    "        if pred in counts:\n",
    "            counts[pred] += 1\n",
    "        else:\n",
    "            counts[pred] = 1\n",
    "    total_count = sum(counts.values())\n",
    "    print({k: v / total_count for k, v in counts.items()})\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a reference point, we evaluate the initial model before training: it gets an accuracy of 32%.\n",
    "\n",
    "To start with, it has very skewed statistics, and almost never predicts a tie, whereas the true label distribution much closer to uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To produce predictions for the validation split:\n",
    "val_token_predictions, val_predictions, val_labels = predict_with_model(peft_model, val_tokens)\n",
    "get_prediction_accuracy(val_labels, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label_distribution(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label_distribution(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To produce predictions for the train split:\n",
    "# train_token_predictions, train_predictions, train_labels = predict_with_model(peft_model, train_tokens)\n",
    "# get_prediction_accuracy(train_labels, train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training model... writing checkpoint to {trainer.args.output_dir} once done\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_token_predictions, val_predictions, val_labels = predict_with_model(peft_model, val_tokens)\n",
    "get_prediction_accuracy(val_labels, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_token_predictions, train_predictions, train_labels = predict_with_model(peft_model, train_tokens)\n",
    "# get_prediction_accuracy(train_labels, train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to an increased validation accuracy (~40%), our model is now giving a less skewed distribution of preferences (your results may vary slightly due to randomness in the training procedure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label_distribution(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label_distribution(val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this prediction task is genuinely very difficult, so it's nice to see that we actually improve on it at least a little!  \n",
    "The prompts as well as the users' subjective preferences may have little in common with each other, and we have trained the model on a very small dataset and on just one epoch.\n",
    "\n",
    "Feel free to experiment with longer training times after getting to the end of the notebook to try to improve this accuracy.  \n",
    "You could also try to increase the number of QLoRA parameters by using 8 or 16 bit floating points, or by increasing the matrix rank `r`, in order to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Although instruction-tuned Llama is quite good at following our instructions, it may occasionally produce other formats of responses, such as `\"Better response: B\"`, especailly when generations are stochastically sampled.\n",
    "How would you change the sampling or evaluation procedure if you would like to restrict the model to answering one of a set of possible responses, defined a priori?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************\n",
    "# YOUR CODE HERE\n",
    "# **********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, how would you restrict the `model.generate()` call to only return 1 new token (A / B / tie are all 1 token long)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************\n",
    "# YOUR CODE HERE\n",
    "# **********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: save predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens_with_predictions = val_tokens.add_column(\n",
    "    f\"{model_id}_4bit_token_predictions\",\n",
    "    [list(pred.cpu().numpy()) for pred in val_token_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out_path = f\"{dataset_name}__{model_id}_4bit_token_predictions\".replace(\"/\", \"__\")\n",
    "val_tokens_with_predictions.save_to_disk(data_out_path)\n",
    "print(\"Wrote dataset to\", data_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending to larger datasets and more training parameters: multi-GPU with model parallel\n",
    "We skipped over several Huggingface features in the above simple fine-tuning setup.\n",
    "\n",
    "In general, you will likely want to batch your inputs, and use several GPUs in parallel. The device_map argument used when loading init_model takes care of loading model parameters across the available devices.\n",
    "\n",
    "When batching, we need to convert the variable length sequences to a fixed length. This can be achieved through either truncating longer sequences and padding shorter ones with padding tokens.\n",
    "\n",
    "If training for multiple epochs, it is also recommended to shuffle the training data between epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative training loop using Huggingface's accelerate library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to use different settings/resources/environments for model training in different phases of your research, different APIs or libraries can provide interfaces to run training:\n",
    "```bash\n",
    "# A single GPU/CPU\n",
    "python your_script.py\n",
    "```\n",
    "or \n",
    "\n",
    "```bash\n",
    "# Multiple GPUs\n",
    "torchrun --nnode=1 --nproc_per_node=4 your_script.py\n",
    "```\n",
    "or \n",
    "\n",
    "```bash\n",
    "# Multiple GPUs\n",
    "deepspeed --num_gpus=4 your_script.py\n",
    "```\n",
    "or\n",
    "\n",
    "......\n",
    "\n",
    "Doing this naively means your_script.py needs to be modified to handle single/multi-GPU training. \n",
    "\n",
    "Is there a better way of doing this? \n",
    "\n",
    "Yes, the accelerate library solves this and ensures the same code can be ran on different computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 8\n",
    "max_length = 512\n",
    "lr = 1e-4\n",
    "num_epochs = 1\n",
    "\n",
    "accelerator.print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens.set_format(type=\"torch\")\n",
    "val_tokens.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_tokens, batch_size=1, shuffle=True)\n",
    "eval_dataloader = DataLoader(val_tokens, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(peft_model.parameters(), lr=lr)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        peft_model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\n",
    "    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        outputs = peft_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            peft_model.zero_grad()\n",
    "\n",
    "    peft_model.eval()\n",
    "    eval_loss = 0\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = peft_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "\n",
    "    # To save model checkpoints:\n",
    "    peft_model.save_pretrained(f\"accelerate_trained_model-{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.cpu().numpy() / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss.cpu().numpy() / len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the trained model:\n",
    "peft_model.save_pretrained(f\"accelerate_trained_model-{epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further use cases\n",
    "\n",
    "Huggingface has a vast library of [tutorials](https://huggingface.co/learn) and resources to keep extending your LLM knowledge.\n",
    "\n",
    "Other recommended topics &ndash; these could be relevant for your team projects:\n",
    "\n",
    "1. Learning from human preferences beyond fine-tuning:  \n",
    " [Reinforcement Learning from Human Feedback](https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf) (RLHF)  \n",
    " [Reward Model training](https://huggingface.co/docs/trl/main/en/reward_trainer)  \n",
    " [Direct Preference Optimization](https://huggingface.co/docs/trl/main/en/dpo_trainer) (DPO)  \n",
    "\n",
    "\n",
    "\n",
    "2. [Retrieval-Augmented Generation](https://huggingface.co/docs/transformers/en/model_doc/rag) (RAG)\n",
    "\n",
    "3. [Distributed Training](https://huggingface.co/docs/transformers/accelerate) (Accelerate)\n",
    "\n",
    "4. [Best practices of LLM prompting](https://huggingface.co/docs/transformers/tasks/prompting)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
