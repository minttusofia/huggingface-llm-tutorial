{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that PyTorch is using the GPU.\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables, this must be done before importing transformers.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if \"TRANSFORMERS_OFFLINE\" in os.environ and int(os.environ[\"TRANSFORMERS_OFFLINE\"]):\n",
    "    print(\"Using cached models from\", os.environ[\"HF_HOME\"])\n",
    "else:\n",
    "    print(\"Loading model from huggingface hub and saving to\", os.environ[\"HF_HOME\"])\n",
    "\n",
    "# Optional: disable warnings.\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Llama-3\n",
    "\n",
    "Meta's Llama-3 is one of the best-performing open-source language models. We will use the 8 billion parameter instruction-tuned version in this tutorial.\n",
    "\n",
    "Llama-3 can be prompted using a chat template: a prompt is a list of messages, each of which has a role (one of \"system\", \"user\" or \"assistant\") and content. System messages are meta-prompts that are used to define instructions that are hidden from the end user, whereas the user and assistant (the LLM) are taking turns in a chat dialogue.\n",
    "\n",
    "**Note:** Make sure to set the `HF_TOKEN` environment variable to access Llama-3 (unless you are running on Puhti or Mahti, where Llama-3-8B-Instruct is already downloaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama-3\n",
    "\n",
    "We will now fine-tune Llama-3-8B on a new dataset. Specifically, we will use human preference data to train the model to better reflect the preferences of human users.\n",
    "\n",
    "Huggingface contains a vast library of datasets, which you can explore at https://huggingface.co/datasets.\n",
    "\n",
    "Instead of fine-tuning all parameters, we will use LoRA, low-rank adaptation, to reduce the computational cost of fine-tuning: https://arxiv.org/abs/2106.09685."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lmsys/lmsys-arena-human-preference-55k\"\n",
    "dataset = load_dataset(dataset_name, split='train[:2000]')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row IDs and model names.\n",
    "# dataset = dataset.remove_columns([\"id\", \"model_a\", \"model_b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_list_from_str_column(text):\n",
    "    lst = []\n",
    "    try:\n",
    "        lst = json.loads(text)\n",
    "        if len(lst) > 0 and lst[-1] is None:\n",
    "            lst = lst[:-1]\n",
    "    except _ as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        print()\n",
    "    return lst\n",
    "\n",
    "def format_prompt(example):\n",
    "    preference_prompt = 'Which response to the following prompt is better? Answer with \"A\", \"B\" or \"tie\" only.'\n",
    "    answer = \"A\" if example[\"winner_model_a\"] else (\"B\" if example[\"winner_model_b\"] else \"tie\")\n",
    "    prompts = parse_list_from_str_column(example[\"prompt\"])\n",
    "    responses_a = parse_list_from_str_column(example[\"response_a\"])\n",
    "    responses_b = parse_list_from_str_column(example[\"response_b\"])\n",
    "    full_prompt = (\n",
    "        preference_prompt\n",
    "        + \"\\n\\nUser prompt:\\n\" + \"\\n\\n\".join(prompts)\n",
    "        + \"\\n\\nResponse A:\\n\" + \"\\n\\n\".join(responses_a)\n",
    "        + \"\\n\\nResponse B:\\n\" + \"\\n\\n\".join(responses_b))\n",
    "    example[\"preference_prompt\"] = full_prompt\n",
    "    example[\"preference_label\"] = answer\n",
    "    return example\n",
    "\n",
    "def is_single_turn_dialogue(example):\n",
    "    prompts = parse_list_from_str_column(example[\"prompt\"])\n",
    "    responses_a = parse_list_from_str_column(example[\"response_a\"])\n",
    "    responses_b = parse_list_from_str_column(example[\"response_b\"])\n",
    "    is_single_turn = len(prompts) == 1 and len(responses_a) == 1 and len(responses_b) == 1\n",
    "    if is_single_turn:\n",
    "        is_null = prompts[0] == \"null\" or responses_a[0] == \"null\" or responses_b[0] == \"null\"\n",
    "        # print(type(prompts[0]), type(responses_a[0]), type(responses_b[0]))\n",
    "        if is_null:\n",
    "            print(\"IS NULL\")\n",
    "            print(prompts, responses_a, responses_b)\n",
    "        return not is_null\n",
    "    return False\n",
    "\n",
    "# Simplify the task to one-turn dialogues.\n",
    "prompt_dataset = dataset.filter(is_single_turn_dialogue)\n",
    "# Format the dataset into (prompt, answer) pairs.\n",
    "prompt_dataset = prompt_dataset.map(format_prompt)\n",
    "print('Original dataset length', len(dataset), 'filtered length', len(prompt_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_dataset[0][\"preference_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_dataset[0][\"preference_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt model before fine-tuning\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_dataset[0][\"preference_prompt\"]},\n",
    "]\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset_split = prompt_dataset.train_test_split(train_size=0.8, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in prompt_dataset_split.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_pipeline(example, pipeline):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": example[\"preference_prompt\"]},\n",
    "    ]\n",
    "    outputs = pipeline(messages)\n",
    "    example[\"initial_prediction\"] = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "    return example\n",
    "\n",
    "pipeline_with_kwargs = functools.partial(\n",
    "    pipeline,\n",
    "    max_new_tokens=1,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "dataset_with_predictions = prompt_dataset.map(\n",
    "    predict_with_pipeline,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    fn_kwargs={\"pipeline\": pipeline_with_kwargs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.utils import send_example_telemetrya\n",
    "\n",
    "# send_example_telemetry(\"question_answering_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(text):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "def tokenize_columns(dataset, text_columns):\n",
    "    for col in text_columns:\n",
    "        dataset[col] = tokenize_fn(dataset[col])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    partial(tokenize_columns, text_columns=('prompt', 'response_a', 'response_b')),\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = tokenized_dataset.train_test_split(test_size=0.3)\n",
    "train_dataset,eval_dataset = split['train'],split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator=DataCollatorForLanguageModeling(tokenizer,mlm=False)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=2, pin_memory=True\n",
    "    )\n",
    "eval_dataloader = DataLoader(\n",
    "        eval_dataset, shuffle=True, collate_fn=data_collator, batch_size=2, pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parallel\n",
    "- Mannually\n",
    "- Huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # First linear layer\n",
    "        self.linear1 = nn.Linear(10000, 10)\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second linear layer\n",
    "        self.linear2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x.to(\"cuda:0\")) # Apply the first linear layer\n",
    "        x = self.relu(x)    # Apply the ReLU activation\n",
    "        x = self.linear2(x.to(\"cuda:1\")) # Apply the second linear layer\n",
    "        return x\n",
    "\n",
    "# Example of creating an instance of the model\n",
    "model = MyModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.linear1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.linear2.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a loss function, for example, Mean Squared Error for a regression task\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer, e.g., Stochastic Gradient Descent, with a learning rate of 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "batch_size = 3\n",
    "input_size = 10000\n",
    "output_size = 5\n",
    "dummy_input = torch.randn(batch_size, input_size)\n",
    "dummy_label = torch.randn(batch_size, output_size).to(next(model.linear2.parameters()).device)\n",
    "\n",
    "# Forward step\n",
    "model.train()  # Set the model to training mode\n",
    "optimizer.zero_grad()  # Clear any gradients from the previous step\n",
    "output = model(dummy_input)  # Compute the model's output\n",
    "loss = loss_function(output, dummy_target)  # Compute the loss\n",
    "\n",
    "# Backward step\n",
    "loss.backward()  # Compute gradients\n",
    "optimizer.step()  # Update parameters\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "class ModelParallelDistilBERT(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ModelParallelDistilBERT, self).__init__()\n",
    "\n",
    "        # Load the configuration and create a DistilBERT model\n",
    "        config = DistilBertConfig.from_pretrained('distilbert-base-uncased', *args, **kwargs)\n",
    "        distilbert = DistilBertModel(config)\n",
    "\n",
    "        # Split the model into two parts\n",
    "        self.part1 = nn.Sequential(\n",
    "            distilbert.embeddings,\n",
    "            *distilbert.transformer.layer[:3]  # First half of the layers\n",
    "        )\n",
    "\n",
    "        self.part2 = nn.Sequential(\n",
    "            *distilbert.transformer.layer[3:]  # Second half of the layers\n",
    "        )\n",
    "\n",
    "        # Place each part on a different GPU\n",
    "        self.part1.cuda(0)\n",
    "        self.part2.cuda(1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Forward pass through the first part\n",
    "        output = self.part1(input_ids.to('cuda:0'), attention_mask.to('cuda:0'))\n",
    "\n",
    "        # Forward pass through the second part\n",
    "        output = self.part2(*output.to('cuda:1'))\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "model = ModelParallelDistilBERT()\n",
    "\n",
    "# Create dummy input data\n",
    "input_ids = torch.randint(0, 30522, (1, 512)).cuda(0)  # Example input token IDs\n",
    "attention_mask = torch.ones((1, 512)).cuda(0)          # Example attention mask\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_ids, attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device_map\n",
    "\"auto\" \"balanced\" ...\n",
    "GPUs>CPU>Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# prompt = \"Once upon a time\"  # Replace with your own prompt\n",
    "# # Encode the prompt\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# # Generate text\n",
    "# output = base_model.generate(input_ids, max_length=20, num_return_sequences=3, no_repeat_ngram_size=2)\n",
    "\n",
    "# # Decode and return the generated text\n",
    "# for text in [tokenizer.decode(generated_id, skip_special_tokens=True) for generated_id in output]:\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import torch\n",
    "\n",
    "\n",
    "#  DataCollatorForSeq2Seq\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from peft import LoraConfig, TaskType, get_peft_model\n",
    "# from peft.utils.other import fsdp_auto_wrap_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT: Parameter-efficient fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#                                                  load_in_8bit=True,\n",
    "#                                                   torch_dtype=torch.float16,\n",
    "#                                                  device_map=\"auto\",\n",
    "                                                  # device_map = {\"\": \"cuda:\" + str(int(os.environ.get(\"LOCAL_RANK\") or 0))}\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, param in enumerate(base_model.named_parameters()):\n",
    "#     print(f'{i},{param[0]}\\t {param[1].device} \\t{param[1].dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the names and shapes of trainable parameters in a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "    model: A Hugging Face model instance.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable_params: {trainable_params}\")\n",
    "    print(f\"all_params: {all_params}\")\n",
    "    \n",
    "print_trainable_parameters(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA: Low-Rank Adaptation of Large Language Models\n",
    "https://arxiv.org/abs/2106.09685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(base_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different training tools from Huggingface**:\n",
    "\n",
    "- Huggingface accelerate library\n",
    "\n",
    "- Trainer: this API supports distributed training on multiple GPUs/TPUs, mixed precision through [NVIDIA Apex] for NVIDIA GPUs, ROCm APEX for AMD GPUs, and Native AMP for PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface accelerate library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to use different settings/resources/environments for model training in different phases of your research, different APIs or libraries can provide interfaces to run training:\n",
    "```bash\n",
    "# A single GPU/CPU\n",
    "python your_script.py\n",
    "```\n",
    "or \n",
    "\n",
    "```bash\n",
    "# Multiple GPUs\n",
    "torchrun --nnode=1 --nproc_per_node=4 your_script.py\n",
    "```\n",
    "or \n",
    "\n",
    "```bash\n",
    "# Multiple GPUs\n",
    "deepspeed --num_gpus=4 your_script.py\n",
    "```\n",
    "or\n",
    "\n",
    "......\n",
    "\n",
    "This often means many lines of code changed. \n",
    "\n",
    "Is there a better way of doing this? \n",
    "\n",
    "Yes, the accelerate library solves this and ensures the same code can be ran on different computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 8\n",
    "max_length = 512\n",
    "lr = 1e-4\n",
    "num_epochs = 3\n",
    "\n",
    "accelerator.print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     print(batch)\n",
    "#     input_ids, attention_mask = batch\n",
    "#     outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#     # Now, you can use outputs for your task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(peft_model.parameters(), lr=lr)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        peft_model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\n",
    "        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(num_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # print(step)\n",
    "        outputs = peft_model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        # print(loss)\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            peft_model.zero_grad()\n",
    "\n",
    "#         capture_batch_analytics(epoch, 'train', step, loss.detach().float(), total_loss, batch[\"input_ids\"], batch[\"labels\"])\n",
    "\n",
    "#     peft_model.eval()\n",
    "#     eval_loss = 0\n",
    "#     for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         eval_loss += loss.detach().float()\n",
    "#         capture_batch_analytics(epoch, 'eval', step, loss.detach().float(), eval_loss, batch[\"input_ids\"], batch[\"labels\"])\n",
    "\n",
    "# #     model.save_pretrained(f\"trained_model-{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device_count = torch.cuda.device_count()\n",
    "# if device_count > 0:\n",
    "# #     logger.debug(\"Select GPU device\")\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "# #     logger.debug(\"Select CPU device\")\n",
    "#     device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=30,\n",
    "  output_dir='out',\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, \n",
    "  logging_steps=1,\n",
    "  gradient_accumulation_steps = 4,\n",
    "  ddp_find_unused_parameters=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer,mlm=False)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_model.save_pretrained(\"./llama_7b_peft\", save_adapter=True, save_config=True)\n",
    "\n",
    "# model_to_merge = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(base_model).to(\"cuda\"), \"./llama_7b_peft\")\n",
    "\n",
    "# merged_model = model_to_merge.merge_and_unload()\n",
    "# merged_model.save_pretrained(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\", \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    \"./llama_7b_peft\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./finetuned_llama2-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  TEST THIS ##########\n",
    "trainer.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
