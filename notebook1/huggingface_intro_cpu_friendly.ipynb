{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables, this must be done before importing transformers.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if \"TRANSFORMERS_OFFLINE\" in os.environ and int(os.environ[\"TRANSFORMERS_OFFLINE\"]):\n",
    "    print(\"Using cached models from\", os.environ[\"HF_HOME\"])\n",
    "else:\n",
    "    print(\"Loading model from huggingface hub and saving to\", os.environ[\"HF_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Huggingface?\n",
    "- Open source.\n",
    "- A vast repository of pre-trained models across various domains.\n",
    "- Compitable with Tensorflow, Pytorch and JAX.\n",
    "- A community, not just a toolkit.\n",
    "- Supports research and engineering.\n",
    "- Fine-tuning capabilities.\n",
    "\n",
    "https://huggingface.co/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Huggingface can be a bit verbose with various warnings. You will see some of them when running this tutorial, but you do not need to worry about them for now.\n",
    "Most of them are quite harmless and are mainly there to help you improve and optimize your Huggingface usage.\n",
    "If you'd like to disable them, you can run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: disable warnings.\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest  approach to use huggingface/transformers for inference: pipeline class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\"sentiment-analysis\")\n",
    "\n",
    "inputs = [\"What a lovely day today!\", \"It is freezing outside.\"]\n",
    "\n",
    "results = pipeline(inputs)\n",
    "\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1**: Try some sentences on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************\n",
    "YOUR_SENTENCES_HERE = [\"\", \"\"]\n",
    "# **********************************************************\n",
    "\n",
    "results = pipeline(YOUR_SENTENCES_HERE)\n",
    "\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\"text-generation\")\n",
    "\n",
    "input_text = \"The capital of France is\"\n",
    "\n",
    "output = pipeline(input_text, truncation=True, max_length=50)\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2**: Can you think of a way to prompt GPT-2 that would make it more likely to continue the text with Paris?\n",
    "\n",
    "For possible arguments, see the `pipeline()` call in the previous cell (or the full list of arguments of the `__call__` function of the `TextGenerationPipeline` class [here](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TextGenerationPipeline.__call__)).  \n",
    "Note that this model is not instruction-tuned, so it won't always be possible to get it to do what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************\n",
    "YOUR_PROMPT_HERE = ...\n",
    "YOUR_ARGUMENTS_HERE = ...\n",
    "# **********************************************************\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\")\n",
    "\n",
    "input_text = YOUR_PROMPT_HERE + \"The capital of France is\"\n",
    "\n",
    "output = pipeline(input_text, **YOUR_ARGUMENTS_HERE)\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompose the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the scenes overview\n",
    "What happens behind the scenes when `pipeline(inputs)` is called:\n",
    "\n",
    "![sentiment analysis pipeline](img/sentiment_analysis_pipeline.svg)\n",
    "\n",
    "The **tokenizer** splits the input into words, subwords, or symbols (like punctuation) that are called *tokens*. It then maps each token to an integer ID, and adds additional inputs that may be useful to the model, such as beginning-of-sequence and end-of-sequence tokens. The token sequence is passed through the model, after which task-specific post-processing is applied.\n",
    "\n",
    "All preprocessing needs to be done in exactly the same way as when the model was pretrained. Conveniently, this information can be accessed from the pipeline instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print relevant tokenizer information\n",
    "print(\"Tokenizer Name:\", pipeline.tokenizer.name_or_path)\n",
    "print(\"Vocabulary Size:\", pipeline.tokenizer.vocab_size)\n",
    "print(\"Max Model Input Sizes:\", pipeline.tokenizer.model_max_length)\n",
    "print(\"Special Tokens:\", pipeline.tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model architecture\n",
    "pipeline.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model config\n",
    "pipeline.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenizers prepare text data for processing by Transformer models. \n",
    "\n",
    "**Tokenizers' function**:\n",
    "\n",
    "1. Text -> tokens: the tokenizer splits the input into words, subwords, or symbols (like punctuation) that become tokens.\n",
    "\n",
    "2. Tokens -> IDs: each token is mapped to a unique integer ID.\n",
    "3. Special tokens are added: \n",
    "    - BERT models use [CLS] at the beginning of the input for classification tasks and [SEP] to separate different segments in the input. \n",
    "        - In model pre-training, certain words in the input are replaced with the [MASK] token. The model then learns to predict the original value of these masked tokens, which helps in learning context and word relationships.\n",
    "    - When the tokenizer encounters a word that is not in its vocabulary, it replaces it with the [UNK] (unknown) token. This is a way to handle out-of-vocabulary words.\n",
    "    - GPT models use [BOS] indicates the start, and [EOS] marks the end of a text sequence. \n",
    "4. Handling Fixed Sequence Lengths: Transformer models require inputs of a fixed length across a batch. Tokenizers pad shorter inputs with [PAD] tokens and truncate longer ones to meet the model's length requirements.\n",
    "\n",
    "5. Attention Mask: The tokenizer generates an attention mask to differentiate real tokens from padding tokens ([PAD]) such that the model will pay attention only to the relevant parts of the input.\n",
    "\n",
    "\n",
    "For multilingual models, tokenizers also ensure consistent tokenization across different languages, maintaining a balanced and shared vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "There are three types of tokenizers: **Word-based, Subword-based, and Character-based**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most state-of-the-art models use subword-based tokenizers:\n",
    "\n",
    "- BERT (Bidirectional Encoder Representations from Transformers): Uses the WordPiece tokenizer.\n",
    "\n",
    "- GPT-2 and GPT-3 (Generative Pre-trained Transformer): Utilize a variant of Byte Pair Encoding (BPE).\n",
    "\n",
    "- T5 (Text-To-Text Transfer Transformer): Employs the SentencePiece tokenizer, which is versatile and can be used across different languages and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, how many GPUs do you need?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, how many GPUs do you need?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, how many GPUs do you need?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text,add_special_tokens=True)\n",
    "print(tokens)\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these tokenizers will typically include a leading space to tokens at the start of words. This may be denoted by `_`, `Ä `, or by the lack of `##` in the example token sequences printed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: A pretrained model only performs properly when the input was tokenized under the same rules that its training data were tokenized.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Classes in Huggingface:\n",
    "- PreTrainedTokenizer: base class for all tokenizers. It provides common methods and attributes that are shared across various tokenizer types. It's not typically used directly for loading specific model tokenizers.\n",
    "- Specifically designed tokenizer, for example: BertTokenizer for the BERT model. It inherits from PreTrainedTokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Directly call a PreTrainedTokenizer, this will throw errors.\n",
    "tokenizer = PreTrainedTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_input = tokenizer(\"Hello, Huggingface!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", padding=True, truncation=True, max_length=20)\n",
    "\n",
    "# Example text\n",
    "text = \"What is the capital of Finland?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters in tokenizer:\n",
    "\n",
    "- padding: padding Strategy\n",
    "- truncate: truncation Strategy\n",
    "- max_length: \n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Call the tokenizer directly when preparing data for model input (like training or inference). The tokenize() method is useful for a token-level inspection or manipulation of the text.\n",
    "\n",
    "Hyperparameters like `padding`, `truncate`, `max_length` are not recognized by tokenize() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Hello, Huggingface! Tell me about all your tokenizer types.\", \"Hello, world!\"]\n",
    "\n",
    "# call a tokenizer directly, invoking its __call__ method\n",
    "encoded_input = tokenizer(text, padding=True, truncation=True, max_length=20) \n",
    "for item in encoded_input.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Huggingface Model Classes:\n",
    "https://huggingface.co/docs/transformers/model_doc/auto\n",
    "- **Base model**:\n",
    "\n",
    "A base model, also referred to as a pretrained model, is a language model that has been trained on a large, generic dataset. The primary purpose of a base model is to capture a wide range of language features and semantics, such as grammar, context, and basic associations. A base model provides a robust foundation of language understanding which can be adapted for specific tasks.\n",
    "\n",
    "Base models in Huggingface are often named after the architecture they use, like bert-base-uncased, gpt2-medium, t5-base, etc.\n",
    "- **Fine-tuned model:**\n",
    "\n",
    "A fine-tuned model is a model that has undergone additional training (fine-tuning) on a smaller, task-specific dataset. This can include tasks like sentiment analysis, question answering, or domain-specific language understanding.\n",
    "\n",
    "Fine-tuned models usually have additional descriptors in their names indicating the specific task or dataset they are fine-tuned for. For instance, **\"bert-base-uncased-finetuned-squad\"** is a BERT model fine-tuned on the SQuAD dataset for question answering, whereas **\"bert-base-uncased\"** is a base model.\n",
    "\n",
    "More information can usually be found in the README or model description in the model repo.  \n",
    "Inspecting the model's configuration or architecture can also give hints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing which model to use\n",
    "[https://huggingface.co/models](https://huggingface.co/models)\n",
    "\n",
    "You may want to consider the following when choosing which model to use:\n",
    "* Task Type\n",
    "* Specific language (especially non-English languages)\n",
    "* Model Size and Performance\n",
    "* Fine-Tuning and Customization\n",
    "* Community and Support\n",
    "* Documentation and Examples\n",
    "* Ethical Considerations\n",
    "* Licensing and Cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the tokenizer, load the model and perform inference, step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Initialize the tokenizer for GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"The capital of France is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate attention mask\n",
    "attention_mask = tokenizer(input_text, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "# Set pad token ID if it's not already set\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do I need to look for the specific tokenizer and model classes for my tasks every time?**\n",
    "\n",
    "In many cases, no. The architecture you want to use can be guessed from the name or the path of the pretrained model. Huggingface provides **AutoClasses** to help you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: AutoModel will instantiate a base model class without a specific head, so we still need \n",
    "# a \"relatively specific\" class AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize the tokenizer for GPT-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"The capital of France is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate attention mask\n",
    "attention_mask = tokenizer(input_text, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "# Set pad token ID if it's not already set\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(input_ids, max_length=50, attention_mask=attention_mask)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key outputs from a language model\n",
    "- Logits: The raw, unnormalized scores for each vocabulary token at each _next_ position in the output sequence. By default, the model's forward pass returns the logits.\n",
    "- Hidden States: Representations from each layer of the model. These are the activations of the model's neurons at each layer. Set `output_hidden_states=True` in the configuration or when calling the model to obtain Hidden States.\n",
    "- Attentions: Attention weights from each layer of the model. These weights show how much each token in a sequence attends to every other token at each layer. Set `output_attentions=True` in the configuration or when calling the model to obtain Attentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"The capital of France is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# get hidden state\n",
    "outputs = model(input_ids)\n",
    "print(outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize the tokenizer for GPT-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare input text\n",
    "input_text = \"The capital of France is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate attention mask\n",
    "attention_mask = tokenizer(input_text, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "# Set pad token ID if it's not already set\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Generate output\n",
    "outputs = model(input_ids, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "print(\"logits:\", outputs.logits)\n",
    "print(\"Attentions:\", outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 3**: What are the dimensions of the output logits? What does each of them correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************\n",
    "# YOUR CODE HERE\n",
    "# **********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 4**: With what probability will the model continue its generation with \"Paris\"?\n",
    "\n",
    "Hint: find out which token IDs the word \"Paris\" consists of, then look up their logits in the model output.  \n",
    "To convert these into a normalized probability distribution per output step, we need to apply a softmax activation over the logits for the same output step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********************************************************\n",
    "# YOUR CODE HERE\n",
    "# **********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration\n",
    "Hyperparameters to change a model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model,GPT2Config\n",
    "\n",
    "# Default configuration\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom configuration\n",
    "config = GPT2Config(\n",
    "    n_layer=6,\n",
    "    n_head=8\n",
    ")\n",
    "# Load model with custom configuration\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", config=config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating/Inference configuration\n",
    "\n",
    "**Different decoding strategies**:\n",
    "\n",
    "https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb\n",
    "\n",
    "**Generation parameters**: \n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/text_generation#transformers.GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "model = \"gpt2\"\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=1.0,\n",
    "    max_length=50,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq[\"generated_text\"]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "You now know how to call language models using the `transformers` library, the steps that are taken under the hood, and have seen how models can be configured.\n",
    "\n",
    "We will now fine-tune an LLM on a new dataset. For the next notebook (`notebook2/llama3_gpu_recommended.ipynb`), you will need a GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
